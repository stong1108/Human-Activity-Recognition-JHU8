{"name":"Jhu-coursera-ml-project","tagline":"Project submission for JHU Coursera ML class","body":"\r\n### Introduction\r\nThe goal of this study is to train a model to correctly identify \"classes\" in which barbell lifts are performed. The dataset (obtained from <http://groupware.les.inf.puc-rio.br/har>) is comprised of 93 variables, mostly accelerometer measurements of participants performing barbell lifts in 5 different ways (classes). \r\n\r\n### Setup\r\nWe will be using the `caret` and the `randomForest` package for this study:\r\n```{r, message = FALSE, warning = FALSE}\r\nrequire(caret)\r\nrequire(randomForest)\r\n```\r\n\r\n### Data Cleaning\r\nFirst we do some cleaning on our dataset. Here, we remove the first 7 variables used for identifying the observation, any variables that are incomplete (columns containing `NA` entries), and any variables that are not numeric.\r\n```{r}\r\ndata <- read.csv(\"pml-training.csv\")\r\nclasse <- data$classe\r\n\r\n# Cleaning\r\ndata <- data[,-c(1:7)]\r\ndata <- data[,colSums(is.na(data))==0]\r\ndata <- data[,sapply(data, is.numeric)]\r\nnumData <- cbind(data, classe)\r\n```\r\n\r\n### Model Training\r\nWe split the dataset into 70% training and 30% testing. Our first model is constructed using random forests because 1) cross-validation is built into the training method since bootstrapping is used for multiple trees, 2) random forests work well for larger numbers of observations (there are 13737 observations in the training set).\r\n\r\nOur first model will be trained using the cleaned dataset containing 53 variables.\r\n```{r}\r\n# Train model\r\ninTrain <- createDataPartition(numData$classe, p = 0.7, list = FALSE)\r\ntraining <- numData[inTrain,]\r\ntesting <- numData[-inTrain,]\r\nmodel <- randomForest(classe~., training)\r\n\r\n# Test model\r\npred <- predict(model, testing)\r\nconfusionMatrix(pred, testing$classe)\r\n```\r\n\r\nHere, we create a second model in which we remove highly correlated variables. The cutoff correlation value that we will use to characterize variables as highly correlated will be set as 0.75.\r\n```{r}\r\n# Create another model with less variables\r\n# Remove highly correlated variables (remove >= 0.75)\r\ncorrMatrix <- cor(data)\r\nhighCorr <- findCorrelation(corrMatrix, cutoff = 0.75)\r\nlowCorrData <- data[,-highCorr]\r\nlowCorrData <- cbind(lowCorrData, classe)\r\n\r\n# Train model\r\ninTrain2 <- createDataPartition(lowCorrData$classe, p = 0.7, list = FALSE)\r\ntraining2 <- lowCorrData[inTrain2,]\r\ntesting2 <- lowCorrData[-inTrain2,]\r\nmodel2 <- randomForest(classe~., training2)\r\n\r\n# Test model\r\npred2 <- predict(model2, testing2)\r\nconfusionMatrix(pred2, testing2$classe)\r\n```\r\n\r\nThis model cuts down on training time with only 32 variables and has the comparable results and performance as the first model on the testing set. Model2 will be used for the rest of the analysis portion of this project. \r\n\r\n### Analysis\r\nBelow is a plot of the variable importance.\r\n```{r fig.width = 3, fig.height = 3, fig.align='center'}\r\nvarImpPlot(model2, cex = 0.5, main = \"Variable Importance\")\r\n```\r\n\r\nAs stated earlier, cross-validation is built into the model, since bootstrapping is used for assessing multiple trees. We can obtain the out-of-sample error from our assessment with the testing data subset.\r\n\r\n```{r}\r\noosError = function(actual, predicted) {\r\n  sum(predicted != actual)/length(actual)\r\n}\r\n\r\noos <- oosError(pred2, testing2$classe)\r\npercent <- round(oos*100, 2)\r\n```\r\nThe out-of-sample error rate for our model is *0.76%*. Yay :D\r\n\r\n### Assignment Portion\r\nThis portion of the project predicts the values of the `pml-testing.csv` dataset and creates text files for submission.\r\n```{r}\r\n# Predict classe of the testing set\r\ntestData <- read.csv(\"pml-testing.csv\")\r\nanswers <- predict(model2, testData)\r\nanswers <- as.character(answers)\r\n\r\n# Write answers to text files\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n\r\npml_write_files(answers)\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}